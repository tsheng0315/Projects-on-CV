# -*- coding: utf-8 -*-
"""Graduate_Admission_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AQznrygN_GQ8Re1Y7Of1o7iIKVNvPDR9

# TASK 2: IMPORT LIBRARIES AND DATASET
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# from jupyterthemes import jtplot
# jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)

# read the csv file 
url='https://raw.githubusercontent.com/tsheng0315/Projects-on-CV/main/University%20Admission%20Prediction%20Using%20Multiple%20Linear%20Regression/data/Admission_Predict.csv'
admission_df=pd.read_csv(url)
admission_df.head()

# Drop the serial no.
admission_df.drop('Serial No.', axis=1,inplace=True)
admission_df

"""# TASK 3: PERFORM EXPLORATORY DATA ANALYSIS"""

# checking the null values
admission_df.isnull().sum()

# Check the dataframe information
admission_df.info()

# Statistical summary of the dataframe
admission_df.describe()

# Grouping by University ranking 
university_df=admission_df.groupby(by='University Rating').mean()
university_df

"""# TASK 4: PERFORM DATA VISUALIZATION"""

admission_df.hist(bins=25,figsize=(20,20),color='red')

sns.pairplot(admission_df)

# admission_df.head()
# sns.pairplot(admission_df
#              ,x_vars=['GRE Score','TOEFL Score','University Rating','SOP','LOR','CGPA','Research','Chance of Admit']
#              ,y_vars=['Chance of Admit'])

corr_admission=admission_df.corr()
plt.figure(figsize=(7,6))
sns.heatmap(corr_admission,annot=True)
plt.show()

"""# TASK 5: CREATE TRAINING AND TESTING DATASET"""

admission_df.columns

X=admission_df.drop('Chance of Admit',axis=1)
#X--> DataFrame

y=admission_df['Chance of Admit']

x=np.array(X)
y=np.array(y)

y=y.reshape(-1,1)
# -1 means unlimited
y.shape

# scaling the data before training the model
from sklearn.preprocessing import StandardScaler, MinMaxScaler
scaler_x=StandardScaler()
x=scaler_x.fit_transform(x)
# x = (x-mean) / stdï¼›
# transform data such that its distribution will have a mean value 0 and standard deviation of 1

scaler_y=StandardScaler()
y=scaler_y.fit_transform(y)

# spliting the data in to test and train sets
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split(x,y,test_size=0.15)

"""# TASK 6: TRAIN AND EVALUATE A LINEAR REGRESSION MODEL"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, accuracy_score
LinearRegression_model=LinearRegression()
LinearRegression_model.fit(x_train,y_train)

accuracy_LinearRegression=LinearRegression_model.score(x_test,y_test)
accuracy_LinearRegression

"""# TASK 7: TRAIN AND EVALUATE AN ARTIFICIAL NEURAL NETWORK"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Activation, Dropout
from tensorflow.keras.optimizers import Adam

ANN_model = keras.Sequential()
ANN_model.add(Dense(50, input_dim = 7))
# 7 variables 
ANN_model.add(Activation('relu'))

ANN_model.add(Dense(150))
ANN_model.add(Activation('relu'))
ANN_model.add(Dropout(0.5))

ANN_model.add(Dense(150))
ANN_model.add(Activation('relu'))
ANN_model.add(Dropout(0.5))

ANN_model.add(Dense(50))
ANN_model.add(Activation('linear'))
ANN_model.add(Dense(1))
# as we are regression model, so avoid sigmoid function etc

ANN_model.compile(loss = 'mse', optimizer = 'adam')

ANN_model.summary()

ANN_model.compile(optimizer='Adam', loss='mean_squared_error')

# fit trainning data to model
epochs_hist = ANN_model.fit(x_train
                            , y_train
                            , epochs = 100
                            , batch_size = 20
                            , validation_split = 0.2)
# 20 samples in one batch
# 100 epochs = train 100 times

result = ANN_model.evaluate(x_test, y_test)
accuracy_ANN = 1 - result
print("Accuracy : {}".format(accuracy_ANN))

# The returned "history" object holds a record of the loss values and metric values during training:
epochs_hist.history.keys()

plt.plot(epochs_hist.history['loss'])
plt.title('Model Loss Progress During Training')
plt.xlabel('Epoch')
plt.ylabel('Training Loss')
plt.legend(['Training Loss'])
plt.show()

"""# TASK 8: TRAIN AND EVALUATE A DECISION TREE AND RANDOM FOREST MODELS"""

# Decision tree builds regression or classification models in the form of a tree structure. 
# Decision tree breaks down a dataset into smaller subsets while at the same time an associated decision tree is incrementally developed. 
# The final result is a tree with decision nodes and leaf nodes.
# Great resource: https://www.saedsayad.com/decision_tree_reg.htm
from sklearn.tree import DecisionTreeRegressor
DecisionTree_model=DecisionTreeRegressor()
DecisionTree_model.fit(x_train,y_train)

accuracy_DecisionTree=DecisionTree_model.score(x_test,y_test)
accuracy_DecisionTree

# Many decision Trees make up a random forest model which is an ensemble model. 
# Predictions made by each decision tree are averaged to get the prediction of random forest model.
# A random forest regressor fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. 
from sklearn.ensemble import RandomForestRegressor

RandomForestRegressor_model=RandomForestRegressor(n_estimators=100,max_depth=10)
# n_estimators: number of trees
RandomForestRegressor_model.fit(x_train,y_train)
accuracy_RandomForestRegressor=RandomForestRegressor_model.score(x_test,y_test)
accuracy_RandomForestRegressor

"""# TASK 10: CALCULATE REGRESSION MODEL KPIs"""

y_predict=LinearRegression_model.predict(x_test)
plt.plot(y_test, y_predict,'^',color='red')
plt.title('Difference between Prediction and Ground Truth')
plt.xlabel('Ground Truth(Test Data)')
plt.ylabel('Prediction')
plt.show()

y_predict_orig= scaler_y.inverse_transform(y_predict)
y_test_orig= scaler_y.inverse_transform(y_test)

plt.plot(y_test_orig, y_predict_orig,'.',color='blue')
plt.title('Difference between Original Prediction and Ground Truth')
plt.xlabel('Original Ground Truth(Test Data)')
plt.ylabel('Original Prediction')
plt.show()

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from math import sqrt

k = x_test.shape[1]
print(x_test.shape)
# number of variables
n = len(x_test)

MAE = mean_absolute_error(y_test_orig, y_predict_orig)
MSE = mean_squared_error(y_test_orig, y_predict_orig)
RMSE = float(format(np.sqrt(mean_squared_error(y_test_orig, y_predict_orig)),'.3f'))

r2 = r2_score(y_test_orig, y_predict_orig)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)

print('\nMAE =',MAE, '\nMSE =',MSE, '\nRMSE =',RMSE, '\nR2 =', r2, '\nAdjusted R2 =', adj_r2)

y_NN_predict=ANN_model.predict(x_test)

y_NN_predict_orig= scaler_y.inverse_transform(y_NN_predict)
y_NN_test_orig= scaler_y.inverse_transform(y_test)

plt.plot(y_NN_test_orig, y_NN_predict_orig,'.',color='blue')
plt.title('Difference between Original Prediction and Ground Truth')
plt.xlabel('Original Ground Truth(Test Data)')
plt.ylabel('Original Prediction')
plt.show()

k = x_test.shape[1]
print(x_test.shape)
# number of variables
n = len(x_test)

MAE_NN = mean_absolute_error(y_NN_test_orig, y_NN_predict_orig)
MSE_NN = mean_squared_error(y_NN_test_orig, y_NN_predict_orig)
RMSE_NN = float(format(np.sqrt(mean_squared_error(y_NN_test_orig, y_NN_predict_orig)),'.3f'))

r2_NN = r2_score(y_NN_test_orig, y_NN_predict_orig)
adj_r2_NN = 1-(1-r2)*(n-1)/(n-k-1)

print('\nMAE_NN =',MAE_NN, '\nMSE_NN =',MSE_NN, '\nRMSE_NN =',RMSE_NN, '\nR2_NN =', r2_NN, '\nAdjusted R2_NN =', adj_r2_NN)

y_RF_predict=RandomForestRegressor_model.predict(x_test)

y_RF_predict_orig= scaler_y.inverse_transform(y_RF_predict)
y_RF_test_orig= scaler_y.inverse_transform(y_test)

plt.plot(y_RF_test_orig, y_RF_predict_orig,'.',color='blue')
plt.title('Difference between Original Prediction and Ground Truth')
plt.xlabel('Original Ground Truth(Test Data)')
plt.ylabel('Original Prediction')
plt.show()

k = x_test.shape[1]
print(x_test.shape)
# number of variables
n = len(x_test)

MAE_RF = mean_absolute_error(y_RF_test_orig, y_RF_predict_orig)
MSE_RF = mean_squared_error(y_RF_test_orig, y_RF_predict_orig)
RMSE_RF = float(format(np.sqrt(mean_squared_error(y_RF_test_orig, y_RF_predict_orig)),'.3f'))

r2_RF = r2_score(y_RF_test_orig, y_RF_predict_orig)
adj_r2_RF = 1-(1-r2)*(n-1)/(n-k-1)

print('\nMAE_RF =',MAE_RF, '\nMSE_RF =',MSE_RF, '\nRMSE_RF =',RMSE_RF, '\nR2_RF =', r2_RF, '\nAdjusted R2_RF =', adj_r2_RF)